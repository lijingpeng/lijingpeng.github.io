<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Frank</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="计算机 网络 互联网">
<meta property="og:type" content="website">
<meta property="og:title" content="Frank">
<meta property="og:url" content="http://www.notehub.cn/page/14/index.html">
<meta property="og:site_name" content="Frank">
<meta property="og:description" content="计算机 网络 互联网">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Frank">
<meta name="twitter:description" content="计算机 网络 互联网">
  
  
    <link rel="icon" href="favicon.png">
  
  <link href='//fonts.css.network/css?family=Open+Sans:400italic,400,600' rel='stylesheet' type='text/css'>
  <link href="//fonts.css.network/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  

  
</head>

<body>
  <div id="container">
    <header id="header">
  <div id="header-main" class="header-inner">
    <div class="outer">
      <a href="/" id="logo"><i class="logo" style="background-image: url(/css/images/logo.jpg)"></i><span class="site-title">Frank</span></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/.">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      
        <nav id="sub-nav">
          <div class="profile" id="profile-nav">
            <a id="profile-anchor" href="javascript:;"><img class="avatar" src="/css/images/logo.png"><i class="fa fa-caret-down"></i></a>
          </div>
        </nav>
      
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"> </button><input type="hidden" name="sitesearch" value="http://www.notehub.cn"></form>
      </div>
    </div>
  </div>
  <div id="main-nav-mobile" class="header-sub header-inner">
    <table class="menu outer">
      <tr>
        
          <td><a class="main-nav-link" href="/.">Home</a></td>
        
          <td><a class="main-nav-link" href="/archives">Archives</a></td>
        
          <td><a class="main-nav-link" href="/categories">Categories</a></td>
        
          <td><a class="main-nav-link" href="/tags">Tags</a></td>
        
          <td><a class="main-nav-link" href="/about">About</a></td>
        
        <td>
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><input type="hidden" name="sitesearch" value="http://www.notehub.cn"></form>
        </td>
      </tr>
    </table>
  </div>
</header>

    <div class="outer">
      
        <aside id="profile">
  <div class="inner profile-inner">
    <div class="base-info profile-block">
      <img id="avatar" src="/css/images/logo.png">
      <h2 id="name">Li Jingpeng</h2>
      <!-- <h3 id="title">undefined</h3> -->
      <span id="location"><i class="fa fa-map-marker"></i>Hangzhou, China</span>
      <a id="follow" href="Https://weibo.com/329299516">关注我</a>
    </div>
    <div class="article-info profile-block">
      <div class="article-info-block">
        101
        <span>文章</span>
      </div>
      <div class="article-info-block">
        41
        <span>标签</span>
      </div>
    </div>
    
    <div class="contact-info profile-block">
      <table class="contact-list">
        <tr>
          
          <td><a href="https://github.com/lijingpeng" target="_blank" title="github"><i class="fa fa-github"></i></a></td>
          
          <td><a href="#" target="_blank" title="twitter"><i class="fa fa-twitter"></i></a></td>
          
          <td><a href="#" target="_blank" title="facebook"><i class="fa fa-facebook"></i></a></td>
          
          <td><a href="/me@lijingpeng.org" target="_blank" title="email"><i class="fa fa-email"></i></a></td>
          
          <td><a href="/atom.xml" target="_blank" title="rss"><i class="fa fa-rss"></i></a></td>
          
        </tr>
      </table>
    </div>
    
    
  </div>
</aside>

      
      <section id="main">
      <article id="post-algo/ml/Feature Processing" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/09/25/algo/ml/Feature Processing/">特征处理</a>
    </h1>
  

        <div class="article-meta">
          
  <div class="article-date">
    <i class="fa fa-calendar"></i>
    <a href="/2015/09/25/algo/ml/Feature Processing/">
      <time datetime="2015-09-24T16:00:00.000Z" itemprop="datePublished">2015-09-25</time>
    </a>
  </div>


          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/machine-learning/">machine learning</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
                
        <p>特征工程（Feature Engineering）经常被说为机器学习中的black art，这里面包含了很多不可言说的方面。怎么处理好特征，最重要的当然还是对要解决问题的了解。但是，它其实也有很多科学的地方。这篇文章我之所以命名为特征处理（Feature Processing），是因为这里面要介绍的东西只是特征工程中的一小部分。这部分比较基础，比较容易说，所以由此开始。单个原始特征（或称为变量）通常属于以下几类之一：</p>
<ul>
<li>连续（continuous）特征；</li>
<li>无序类别（categorical）特征；</li>
<li>有序类别（ordinal）特征。</li>
</ul>
<p>本文中我主要介绍针对单个特征的处理方法，虽然也会附带介绍基础的特征组合方法。同时处理多个特征，以及更复杂的特征处理方法介绍，以后我再另外细说。下面我由浅入深地逐渐说明针对这三类特征的常用处理方法。</p>
<h2 id="u521D_u7EA7_u7BC7"><a href="#u521D_u7EA7_u7BC7" class="headerlink" title="初级篇"></a>初级篇</h2><hr>
<h3 id="u8FDE_u7EED_u7279_u5F81"><a href="#u8FDE_u7EED_u7279_u5F81" class="headerlink" title="连续特征"></a>连续特征</h3><p>除了归一化（去中心，方差归一），不用做太多特殊处理，可以直接把连续特征扔到模型里使用。</p>
<h3 id="u65E0_u5E8F_u7279_u5F81"><a href="#u65E0_u5E8F_u7279_u5F81" class="headerlink" title="无序特征"></a>无序特征</h3><p>可以使用One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如：</p>
<pre><code>color取值 向量表示
red     (1, 0, 0)
green   (0, 1, 0)
blue    (0, 0, 1)
</code></pre><p>这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。</p>
<p>机器学习书籍里在讲这个的时候介绍的处理方法可能跟我上面说的有点差别。上面说的表达方式里有一个维度是可以省略的。既然我们知道color一定是取3个值中的一个，那么我们知道向量的前两个元素值，就能推断第3个值是多少。所以，其实用下面的方式就可以表达到底是哪种颜色：</p>
<pre><code>color取值 向量表示
red     (1, 0)
green   (0, 1)
blue    (0, 0)
</code></pre><p>这样表达的好处是少用了一个维度，降低了转化后特征之间的相关性。但在实际问题中特征基本都或多或少会有些缺失。使用第一种表达方式就可以用全0的向量来表示值缺失，而第二种表达方式是没法表达缺失的。</p>
<h3 id="u6709_u5E8F_u7279_u5F81"><a href="#u6709_u5E8F_u7279_u5F81" class="headerlink" title="有序特征"></a>有序特征</h3><p>有些特征虽然也像无序特征那样只取限定的几个值，但是这些值之间有顺序的含义。例如一个人的状态status有三种取值：bad, normal, good，显然bad &lt; normal &lt; good。</p>
<p>当然，对有序特征最简单的处理方式是忽略其中的顺序关系，把它看成无序的，这样我们就可以使用处理无序特征的方式来处理它。在实际问题中，这种处理方式其实用的很多。</p>
<p>当然有些问题里有序可能会很重要，这时候就不应该把其中的顺序关系丢掉。一般的表达方式如下：</p>
<pre><code>status取值    向量表示
bad     (1, 0, 0)
normal  (1, 1, 0)
good    (1, 1, 1)
</code></pre><p>上面这种表达方式很巧妙地利用递进表达了值之间的顺序关系。</p>
<h2 id="u4E2D_u7EA7_u7BC7"><a href="#u4E2D_u7EA7_u7BC7" class="headerlink" title="中级篇"></a>中级篇</h2><hr>
<p>最容易让人掉以轻心的，往往就是大家觉得最简单的事。在特征处理中，最容易让刚入门同学忽略的，是对连续特征的处理方式。</p>
<p>以线性分类器Linear Regression (LinearReg)为例，它是通过特征的线性加权来预测因变量y：</p>
<pre><code>y=wTx
</code></pre><p>但大部分实际情况下，y与x都不会是这么简单的线性关系，甚至连单调关系都不会有。举个只有一个特征的例子，如果y与x的实际关系如下图：</p>
<p><img src="/images/algo/nonlinear_function1.png" alt=""></p>
<p>那么直接把x扔进LinearReg模型是怎么也得不到好结果的。很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是Logistic Regression (LogisticReg)。</p>
<p>对于上面这个问题，有没有什么办法使得LinearReg也能处理得不错？当然是有，就是对原始特征x做转化，把原来的非线性关系转化为线性关系。</p>
<h3 id="u65B9_u6CD5_u4E00_uFF1A_u79BB_u6563_u5316"><a href="#u65B9_u6CD5_u4E00_uFF1A_u79BB_u6563_u5316" class="headerlink" title="方法一：离散化"></a>方法一：离散化</h3><p>最常用的转化方式是对x做离散化(discretization)，也就是把原来的值分段，转化成一个取值为0或1的向量。原始值落在某个段里，向量中此段对应的元素就为1，否则为0。</p>
<p>离散化的目标是y与转化后向量里的每个元素都保持比较好的线性关系。<br>比如取离散点{0.5,1.5,2.5}，通过判断x属于(−∞,0.5)，[0.5,1.5)，[1.5,2.5)，[2.5,+∞)中哪段来把它离散化为4维的向量。下面是一些例子的离散结果：</p>
<pre><code>原始值x    离散化后的值
0.1     (1, 0, 0, 0)
1.3     (0, 1, 0, 0)
3.2     (0, 0, 0, 1)
5.8     (0, 0, 0, 1)
</code></pre><p>离散化方法的关键是怎么确定分段中的离散点。下面是常用的选取离散点的方法：</p>
<p>a. 等距离离散：顾名思义，就是离散点选取等距点。我们上面对x取离散点{0.5,1.5,2.5}就是一种等距离散，见下图。图中垂直的灰线代表离散点。</p>
<p><img src="/images/algo/nonlinear_function2.png" alt=""></p>
<p>b. 等样本点离散：选取的离散点保证落在每段里的样本点数量大致相同，见下图。</p>
<p><img src="/images/algo/nonlinear_function3.png" alt=""></p>
<p>c. 画图观察趋势：以x为横坐标，y为纵坐标，画图，看曲线的趋势和拐点。通过观察下面的图我们发现可以利用3条直线（红色直线）来逐段近似原来的曲线。把离散点设为两条直线相交的各个点，我们就可以把x离散化为长度为3的向量。</p>
<p><img src="/images/algo/nonlinear_function4.png" alt=""></p>
<p>上面介绍的这种离散化为0/1向量的方法有个问题，它在离散时不会考虑到具体的x到离散边界的距离。比如等距离散中取离散点为{0.5,1.5,2.5}，那么1.499，1.501和2.49分别会离散为(0, 1, 0, 0)，(0, 0, 1, 0)和(0, 0, 1, 0)。1.499和1.501很接近，可是就因为这种强制分段的离散导致它们离散的结果差距很大。</p>
<p>针对上面这种硬离散的一种改进就是使用软离散，也就是在离散时考虑到x与附近离散点的距离，离散出来的向量元素值可以是0/1之外的其他值。有兴趣的同学可以去ESL1这本书中找点感觉。</p>
<h3 id="u65B9_u6CD5_u4E8C_uFF1A_u51FD_u6570_u53D8_u6362"><a href="#u65B9_u6CD5_u4E8C_uFF1A_u51FD_u6570_u53D8_u6362" class="headerlink" title="方法二：函数变换"></a>方法二：函数变换</h3><p>函数变换直接把原来的特征通过非线性函数做变换，然后把原来的特征，以及变换后的特征一起加入模型进行训练。常用的变换函数见下表，不过其实你可以尝试任何函数。</p>
<pre><code>常用非线性函数f(x) x的取值范围
xα; α∈(−∞,+∞)   (−∞,+∞)
log(x)          (0,+∞)
log(x1−x)       (0,1)
</code></pre><p>这个方法操作起来很简单，但记得对新加入的特征做归一化。</p>
<p>对于我们前面的问题，只要把x2，x3也作为特征加入即可，因为实际上y就是x的一个三次多项式。</p>
<h2 id="u9AD8_u7EA7_u7BC7"><a href="#u9AD8_u7EA7_u7BC7" class="headerlink" title="高级篇"></a>高级篇</h2><hr>
<h3 id="u7B1B_u5361_u5C14_u4E58_u79EF"><a href="#u7B1B_u5361_u5C14_u4E58_u79EF" class="headerlink" title="笛卡尔乘积"></a>笛卡尔乘积</h3><p>我们可以使用笛卡尔乘积的方式来组合2个或更多个特征。比如有两个类别特征color和light，它们分别可以取值为red，green，blue和on, off。这两个特征各自可以离散化为3维和2维的向量。对它们做笛卡尔乘积转化，就可以组合出长度为6的特征，它们分别对应着原始值对(red, on)，(red, off)，(green, on)，(green, off)，(blue, on)，(blue, off)。下面的矩阵表达方式更清楚地说明了这种组合。</p>
<pre><code>X       on  off
red      
green        
blue
</code></pre><p>对于3个特征的笛卡尔乘积组合，可以表达为立方的形式。更多特征的组合依次类推。 这个方法也可以直接用于连续特征与类别特征之间的组合，只要把连续特征看成是1维的类别特征就好了，这时候组合后特征对应的值就不是0/1了，而是连续特征的取值。</p>
<h3 id="u79BB_u6563_u5316_u7EED_u7BC7"><a href="#u79BB_u6563_u5316_u7EED_u7BC7" class="headerlink" title="离散化续篇"></a>离散化续篇</h3><p>在上节中我已经介绍了一些常用的离散化单个连续特征的方法，其中一个是画图观察趋势。画图观察趋势的好处是直观、可解释性强，坏处是很麻烦。当要离散化的特征很多时，这种方法可操作性较差。</p>
<p>机器学习中有个很好解释，速度也不错的模型——决策树模型。大白话说决策树模型就是一大堆的if else。它天生就可以对连续特征分段，所以把它用于离散化连续特征合情合理。我称这种方法为决策树离散化方法。例如Gmail在对信件做重要性排序时就使用了决策树离散化方法2。</p>
<p>决策树离散化方法通常也是每次离散化一个连续特征，做法如下：</p>
<p>单独用此特征和目标值y训练一个决策树模型，然后把训练获得的模型内的特征分割点作为离散化的离散点。<br>这种方法当然也可以同时离散化多个连续特征，但是操作起来就更复杂了，实际用的不多。</p>
<h3 id="u6838_u65B9_u6CD5"><a href="#u6838_u65B9_u6CD5" class="headerlink" title="核方法"></a>核方法</h3><p>核方法经常作为线性模型的一种推广出现。以线性回归模型为例，它对应的核方法如下：</p>
<pre><code>fθ(x)=∑θiK(x,xi)
</code></pre><p>其中{xi}=1为训练样本点，K(xi,xj)为核函数，比如常用的高斯核函数为：</p>
<pre><code>K(xi,xj)=exp(−exp(∥xi−xj∥, 2)/2*exp(h,2))
</code></pre><p>如果我们把上面模型里的{K(x,xi)}=1看成特征，而θ看成模型参数的话，上面的模型仍旧是个线性模型。所以可以认为核方法只是特征函数变换的一种方式。</p>
<p>当然，如果把核函数K(xi,xj)看成一种相似度的话，那上面的模型就是kNN模型了，或者叫做加权平均模型也可以。因为核方法在预测时也要用到训练样本点，耗内存且计算量大，所以在数据量较大的实际问题中用的并不多。到此，我已经介绍了不少针对单个特征的处理方法。这些处理方法很难说哪个好哪个不好。有些问题这个好，有些问题那个好，也没什么绝招能直接判断出哪种方法能适合哪些问题。唯一的招就是：</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.notehub.cn/2015/09/25/algo/ml/Feature Processing/" data-id="ciszf7pu0006re4s8eds7cktk" class="article-share-link">分享到</a>
      
        <a href="http://www.notehub.cn/2015/09/25/algo/ml/Feature Processing/#ds-thread" class="article-comment-link">评论</a>
      
      
    </footer>
  </div>
  
</article>



    
      <article id="post-algo/ml/feature_hashing" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/09/25/algo/ml/feature_hashing/">Feature hashing</a>
    </h1>
  

        <div class="article-meta">
          
  <div class="article-date">
    <i class="fa fa-calendar"></i>
    <a href="/2015/09/25/algo/ml/feature_hashing/">
      <time datetime="2015-09-24T16:00:00.000Z" itemprop="datePublished">2015-09-25</time>
    </a>
  </div>


          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/machine-learning/">machine learning</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
                
        <p>In machine learning, feature hashing, also known as the hashing trick(by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array.</p>
<h3 id="Motivating_example"><a href="#Motivating_example" class="headerlink" title="Motivating example"></a>Motivating example</h3><p>In a typical document classification task, the input to the machine learning algorithm (both during learning and classification) is free text. From this, a bag of words (BOW) representation is constructed: the individual tokens are extracted and counted, and each distinct token in the training set defines a feature (independent variable) of each of the documents in both the training and test sets.</p>
<p>Machine learning algorithms, however, are typically defined in terms of numerical vectors. Therefore, the bags of words for a set of documents is regarded as a term-document matrix where each row is a single document, and each column is a single feature/word; the entry i, j in such a matrix captures the frequency (or weight) of the j’th term of the vocabulary in document i. (An alternative convention swaps the rows and columns of the matrix, but this difference is immaterial.) Typically, these vectors are extremely sparse.</p>
<p>The common approach is to construct, at learning time or prior to that, a dictionary representation of the vocabulary of the training set, and use that to map words to indices. Hash tables and tries are common candidates for dictionary implementation. E.g., the three documents</p>
<p>John likes to watch movies.<br>Mary likes movies too.<br>John also likes football.<br>can be converted, using the dictionary</p>
<pre><code>Term    Index
John    1
likes   2
to      3
watch   4
movies  5
Mary    6
too     7
also    8
football9
</code></pre><p>to the term-document matrix</p>
<pre><code>1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 
0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1
</code></pre><p>(Punctuation was removed, as is usual in document classification and clustering.)</p>
<p>The problem with this process is that such dictionaries take up a large amount of storage space and grow in size as the training set grows. On the contrary, if the vocabulary is kept fixed and not increased with a growing training set, an adversary may try to invent new words or misspellings that are not in the stored vocabulary so as to circumvent a machine learned filter. This difficulty is why feature hashing has been tried for spam filtering at Yahoo! Research.</p>
<p>Note that the hashing trick isn’t limited to text classification and similar tasks at the document level, but can be applied to any problem that involves large (perhaps unbounded) numbers of features.</p>
<h3 id="Feature_vectorization_using_the_hashing_trick"><a href="#Feature_vectorization_using_the_hashing_trick" class="headerlink" title="Feature vectorization using the hashing trick"></a>Feature vectorization using the hashing trick</h3><p>Instead of maintaining a dictionary, a feature vectorizer that uses the hashing trick can build a vector of a pre-defined length by applying a hash function h to the features (e.g., words) in the items under consideration, then using the hash values directly as feature indices and updating the resulting vector at those indices:</p>
<p> function hashing_vectorizer(features : array of string, N : integer):</p>
<pre><code>     x := new vector[N]
     for f in features:
         h := hash(f)
         x[h mod N] += 1
     return x
</code></pre><p>It has been suggested that a second, single-bit output hash function ξ be used to determine the sign of the update value, to counter the effect of hash collisions. If such a hash function is used, the algorithm becomes</p>
<p> function hashing_vectorizer(features : array of string, N : integer):</p>
<pre><code>     x := new vector[N]
     for f in features:
         h := hash(f)
         idx := h mod N
         if ξ(f) == 1:
             x[idx] += 1
         else:
             x[idx] -= 1
     return x
</code></pre><p>The above pseudocode actually converts each sample into a vector. An optimized version would instead only generate a stream of (h,ξ) pairs and let the learning and prediction algorithms consume such streams; a linear model can then be implemented as a single hash table representing the coefficient vector.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.notehub.cn/2015/09/25/algo/ml/feature_hashing/" data-id="ciszf7pu3006ve4s8eytk34ni" class="article-share-link">分享到</a>
      
        <a href="http://www.notehub.cn/2015/09/25/algo/ml/feature_hashing/#ds-thread" class="article-comment-link">评论</a>
      
      
    </footer>
  </div>
  
</article>



    
      <article id="post-algo/ml" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/09/23/algo/ml/">机器学习资料大汇总</a>
    </h1>
  

        <div class="article-meta">
          
  <div class="article-date">
    <i class="fa fa-calendar"></i>
    <a href="/2015/09/23/algo/ml/">
      <time datetime="2015-09-22T16:00:00.000Z" itemprop="datePublished">2015-09-23</time>
    </a>
  </div>


          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/machine-learning/">machine learning</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
                
        <p><img src="/images/other/ml.jpg" alt=""></p>
<p>注：本页面主要针对想快速上手机器学习而又不想深入研究的同学，对于专门的researcher，建议直接啃PRML，ESL，MLAPP以及你相应方向的书（比如Numerical Optimization，Graphic Model等），另外就是Follow牛会牛paper，如果谁有兴趣也可以一起来整理个专业的汇总页。本页面将持续更新，敬请关注，如有推荐的文章请留言，谢谢！</p>
<h3 id="u5F00_u6E90_u5DE5_u5177"><a href="#u5F00_u6E90_u5DE5_u5177" class="headerlink" title="开源工具"></a>开源工具</h3><hr>
<p><a href="http://www.52ml.net/12043.html" target="_blank" rel="external">机器学习的开源工具</a><br><a href="http://www.52ml.net/13547.html" target="_blank" rel="external">Python机器学习库</a><br><a href="http://www.52ml.net/13002.html" target="_blank" rel="external">C++矩阵运算库推荐</a></p>
<h3 id="u516C_u5F00_u8BFE"><a href="#u516C_u5F00_u8BFE" class="headerlink" title="公开课"></a>公开课</h3><hr>
<ul>
<li>Machine Learning | Coursera Andrew NG在coursera上的课，难度比公开课略低，适合入门</li>
<li>斯坦福大学公开课 ：机器学习课程 Andrew NG在学校里面的课程，网易公开课有中英文字幕，可以配合笔记来看</li>
<li>CMU机器学习系主任Tom Mitchell院士机器学习课程视频及课件（英文）</li>
<li>机器学习|加州理工，老师是Yaser Abu-Mostafa，会从最基本的理论开始，为你构建机器学习的基础。</li>
<li>机器学习基石 如果想听中文课程，台湾大学的这门就很合适，友情提示，台大的课程基本上都可以加快语速来听，原因你懂的</li>
<li>神经网络|多伦多大学 鼎鼎大名的Geoffrey Hinton ，这门课着实不容错过</li>
<li>凸优化课程|斯坦福 授课老师是凸优化经典教材的作者Stephen Boyd！有难度有挑战！</li>
<li>概率图模型  coursera的另外一个创始人，Daphne Koller的课程，值得一提的是，Koller因提出了Probabilistic Relational Models拿到了2001年的IJCAI Computers and Thought Award</li>
<li>统计学习|斯坦福 授课老师是ESL作者 ，还有同学把视频放在了百度网盘上～ 这个更快一些</li>
</ul>
<h3 id="1-__u673A_u5668_u5B66_u4E60_u5165_u95E8_u7BC7"><a href="#1-__u673A_u5668_u5B66_u4E60_u5165_u95E8_u7BC7" class="headerlink" title="1. 机器学习入门篇"></a>1. 机器学习入门篇</h3><h4 id="1-1__u673A_u5668_u5B66_u4E60_u4ECB_u7ECD"><a href="#1-1__u673A_u5668_u5B66_u4E60_u4ECB_u7ECD" class="headerlink" title="1.1 机器学习介绍"></a>1.1 机器学习介绍</h4><ul>
<li>机器学习-维基百科  Machine Learning-Wikipedia</li>
<li>机器学习简史</li>
<li>规则与机器学习 不建议为了机器学习而机器学习，对于初学者应该是先规则再机器学习，规则直观，可以深入理解领域知识和特征，要记住一个机器学习的专家必须首先是该领域知识的专家。</li>
<li>贝叶斯思想 MLAPP 第5章 Bayesian statistics 第6章 Frequentist statistics 机器学习第6章 贝叶斯学习</li>
<li>监督学习 ESL 第2章 Overview of Supervised Learning</li>
</ul>
<h4 id="1-2__u4E66_u7C4D"><a href="#1-2__u4E66_u7C4D" class="headerlink" title="1.2 书籍"></a>1.2 书籍</h4><ul>
<li>《统计学习方法》 第1章 统计学习方法概论</li>
<li>《机器学习》（Mitchell） 第1章 引言</li>
<li>PRML 第1章 Introduction</li>
<li>MLAPP 第1章 Introduction 第2章 Probability</li>
<li>ESL 第1章 Introduction</li>
<li>Some Notes on Applied Mathematics for Machine (选修)</li>
<li>Machine Learning Textbook minireviews</li>
<li>List of Cool Machine Learning Books</li>
</ul>
<h4 id="1-3__u6570_u5B66_u57FA_u7840"><a href="#1-3__u6570_u5B66_u57FA_u7840" class="headerlink" title="1.3 数学基础"></a>1.3 数学基础</h4><ul>
<li>线性代数：公开课： 线性代数；推荐文章 ： 线性代数的本质，</li>
<li>概率论：公开课： 概率课|台大 叶老师为人风趣幽默，课程也比较简单，容易听进去</li>
<li>书籍：MLAPP第二章</li>
<li>微积分：公开课：单变量微积分|MIT 多变量微积分|MIT</li>
</ul>
<p>——————————————-</p>
<h4 id="1-4_LDA"><a href="#1-4_LDA" class="headerlink" title="1.4 LDA"></a>1.4 LDA</h4><ul>
<li>LDA最佳学习资料汇总</li>
</ul>
<h4 id="1-4_Spectral_Clustering"><a href="#1-4_Spectral_Clustering" class="headerlink" title="1.4 Spectral Clustering"></a>1.4 Spectral Clustering</h4><ul>
<li>Spectral Clustering最佳学习资料汇总</li>
</ul>
<h4 id="1-5__u56FE_u50CF_u5904_u7406"><a href="#1-5__u56FE_u50CF_u5904_u7406" class="headerlink" title="1.5 图像处理"></a>1.5 图像处理</h4><ul>
<li>图像处理和计算机视觉中的经典论文</li>
</ul>
<h4 id="2__u7EBF_u6027_u56DE_u5F52_u6A21_u578B"><a href="#2__u7EBF_u6027_u56DE_u5F52_u6A21_u578B" class="headerlink" title="2 线性回归模型"></a>2 线性回归模型</h4><ul>
<li>PRML 第3章 Linear Models for Regression</li>
<li>MLAPP 第7章 Linear Regression 第13章 Sparse Linear Models</li>
<li>ESL 第3章 Linear Method for Regression</li>
</ul>
<h4 id="3__u7EBF_u6027_u5206_u7C7B_u6A21_u578B"><a href="#3__u7EBF_u6027_u5206_u7C7B_u6A21_u578B" class="headerlink" title="3 线性分类模型"></a>3 线性分类模型</h4><ul>
<li>PRML 第4章 Linear Models for Classification</li>
<li>MLAPP 第8章 Logistic Regression 第9章 Generalized Linear Models and the exponential family</li>
<li>ESL 第4章 Linear Method for Classification</li>
<li>统计机器学习 第6章 逻辑斯谛回归与最大熵模型</li>
</ul>
<h4 id="4__u795E_u7ECF_u7F51_u7EDC"><a href="#4__u795E_u7ECF_u7F51_u7EDC" class="headerlink" title="4 神经网络"></a>4 神经网络</h4><ul>
<li>PRML 第5章 Neural Networks</li>
<li>ESL 第11章 Neural Networks</li>
<li>统计学习方法 第2章 感知机</li>
<li>机器学习 第4章 人工神经网络</li>
</ul>
<h4 id="5__u652F_u6301_u5411_u91CF_u673A"><a href="#5__u652F_u6301_u5411_u91CF_u673A" class="headerlink" title="5 支持向量机"></a>5 支持向量机</h4><ul>
<li>统计学习方法 第7章 支持向量机 (强烈推荐)</li>
<li>PRML 第6章 Kernel Methods 第7章 Sparse Kernel Machine</li>
<li>ESL 第12章 Support Vector Machines and Flexible Discriminants</li>
<li>MLAPP 第14章 Kernels</li>
</ul>
<h4 id="6__u56FE_u6A21_u578B"><a href="#6__u56FE_u6A21_u578B" class="headerlink" title="6 图模型"></a>6 图模型</h4><ul>
<li>PRML 第8章 Graphical Models</li>
<li>MLAPP 第10章 Directed graphical models（Bayes nets） 第19章 Undirected Graphical Models（Marcov random fields）第20章 Exact inference for graphical models 第26章 Graphical model structure learning</li>
<li>统计学习方法 第10章 隐马尔可夫模型 第11章 条件随机场</li>
<li>机器学习 6.11 贝叶斯信念网</li>
<li>ESL 第17章 Undirected Graphical Models</li>
<li>Koller 的书</li>
<li>Jordan 的书</li>
</ul>
<h4 id="7__u6DF7_u5408_u6A21_u578B_u548CEM"><a href="#7__u6DF7_u5408_u6A21_u578B_u548CEM" class="headerlink" title="7 混合模型和EM"></a>7 混合模型和EM</h4><ul>
<li>PRML 第9章 Mixture Models and EM</li>
<li>MLAPP 第11章 Mixture models and the EM algorithm</li>
<li>ESL 8.5 The EM Algorithm</li>
<li>统计学习方法 第9章 EM算法及其推广</li>
</ul>
<h4 id="8__u8FD1_u4F3C_u63A8_u7406"><a href="#8__u8FD1_u4F3C_u63A8_u7406" class="headerlink" title="8 近似推理"></a>8 近似推理</h4><ul>
<li>PRML 第10章 Approximate Inference</li>
<li>MLAPP 第21章 Variational Inference 第22章 More Variational Inference</li>
</ul>
<h4 id="9__u91C7_u6837_u65B9_u6CD5"><a href="#9__u91C7_u6837_u65B9_u6CD5" class="headerlink" title="9 采样方法"></a>9 采样方法</h4><ul>
<li>PRML 第11章 Sampling Methods</li>
<li>MLAPP 第23章 Monte Carlo inference 第24章 Markov Chain Monte Carlo (MCMC) inference</li>
<li>ESL 8.6 MCMC for Sampling from Posterior</li>
</ul>
<h4 id="10_PCA"><a href="#10_PCA" class="headerlink" title="10 PCA"></a>10 PCA</h4><ul>
<li>PRML 第12章 Continuous Latent Variables</li>
<li>MLAPP 第12章 Latent Linear Models</li>
<li>ESL 14.5 Principal Componens， Curves and Surfaces</li>
</ul>
<h4 id="11_HMM"><a href="#11_HMM" class="headerlink" title="11 HMM"></a>11 HMM</h4><ul>
<li>PRML 13.1 13.2 Hidden Marcov Models</li>
<li>MLAPP 第17章 Marcov and Hidden Marcov Models</li>
</ul>
<h4 id="12__u7EC4_u5408_u6A21_u578B"><a href="#12__u7EC4_u5408_u6A21_u578B" class="headerlink" title="12 组合模型"></a>12 组合模型</h4><ul>
<li>(投票，boosting，bagging，树模型，model averaging)</li>
<li>PRML 第14章 Combining Models</li>
<li>统计学习方法 第5章 决策树 第8章 提升方法</li>
<li>MLAPP 第16章 Adaptive basis function models</li>
<li>ESL 第15章 Random Forests 第16章 Ensemble Learning 8.7 Bagging 第9章 Additive Models, Trees, and Related Methods 第10章 Boosting and Additive Trees</li>
<li>机器学习 第3章 决策树学习</li>
</ul>
<h4 id="14__u805A_u7C7B"><a href="#14__u805A_u7C7B" class="headerlink" title="14 聚类"></a>14 聚类</h4><ul>
<li>ESL 14.3 Cluster Analysis</li>
<li>MLAPP 25章 Clustering</li>
<li>PRML 9.1 K-means Clustering</li>
</ul>
<h4 id="15__u8FD1_u90BB"><a href="#15__u8FD1_u90BB" class="headerlink" title="15 近邻"></a>15 近邻</h4><ul>
<li>ELS 第13章 Protype Methods and Nearest-Neighbors</li>
</ul>
<h4 id="16_Deep_Learning"><a href="#16_Deep_Learning" class="headerlink" title="16 Deep Learning"></a>16 Deep Learning</h4><ul>
<li><a href="http://deeplearning.net/" target="_blank" rel="external">http://deeplearning.net/</a></li>
<li>Deep Learning Tutorial</li>
<li>MLAPP 第28章 Deep Learning</li>
</ul>
<h4 id="2-2_Deep_Learning_u6559_u7A0B"><a href="#2-2_Deep_Learning_u6559_u7A0B" class="headerlink" title="2.2 Deep Learning教程"></a>2.2 Deep Learning教程</h4><ul>
<li>UFLDL-斯坦福大学Andrew Ng教授“Deep Learning”教程</li>
</ul>
<h3 id="3-__u81EA_u7136_u8BED_u8A00_u5904_u7406_u5165_u95E8_u7BC7"><a href="#3-__u81EA_u7136_u8BED_u8A00_u5904_u7406_u5165_u95E8_u7BC7" class="headerlink" title="3. 自然语言处理入门篇"></a>3. 自然语言处理入门篇</h3><h4 id="3-1__u65AF_u5766_u798F_u5927_u5B66_u81EA_u7136_u8BED_u8A00_u5904_u7406_u516C_u5F00_u8BFE"><a href="#3-1__u65AF_u5766_u798F_u5927_u5B66_u81EA_u7136_u8BED_u8A00_u5904_u7406_u516C_u5F00_u8BFE" class="headerlink" title="3.1 斯坦福大学自然语言处理公开课"></a>3.1 斯坦福大学自然语言处理公开课</h4><ul>
<li>NLP | 斯坦福  授课教师是 Dan Jurafsky 以及 Christopher Manning，英文不是很有信心的可以参考《斯坦福大学自然语言处理公开课中文解读》</li>
<li>NLP | 哥伦比亚 授课老师是Michael Collins大神</li>
</ul>
<h4 id="3-2__u7EDF_u8BA1_u673A_u5668_u7FFB_u8BD1"><a href="#3-2__u7EDF_u8BA1_u673A_u5668_u7FFB_u8BD1" class="headerlink" title="3.2 统计机器翻译"></a>3.2 统计机器翻译</h4><ul>
<li>Statistical Machine Translation</li>
<li>统计机器翻译开源软件汇总</li>
</ul>
<p>转自：<a href="http://www.52ml.net/star?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io" target="_blank" rel="external">http://www.52ml.net/star?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.notehub.cn/2015/09/23/algo/ml/" data-id="ciszf7pq7000ue4s89btsu9z0" class="article-share-link">分享到</a>
      
        <a href="http://www.notehub.cn/2015/09/23/algo/ml/#ds-thread" class="article-comment-link">评论</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PPTP-vpn/">PPTP, vpn</a></li></ul>

    </footer>
  </div>
  
</article>



    
      <article id="post-dev/How to customize Writable class in Hadoop" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/09/21/dev/How to customize Writable class in Hadoop/">How to customize Writable class in Hadoop</a>
    </h1>
  

        <div class="article-meta">
          
  <div class="article-date">
    <i class="fa fa-calendar"></i>
    <a href="/2015/09/21/dev/How to customize Writable class in Hadoop/">
      <time datetime="2015-09-21T04:49:45.000Z" itemprop="datePublished">2015-09-21</time>
    </a>
  </div>


          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
                
        <p>I’m trying to implement Writable class, but i have no idea on how to implement a writable class if in my class there is nested object, such as list, etc.</p>
<pre><code class="java">public class StorageClass implements Writable{

public String xStr;
public String yStr;

public List&lt;Field&gt; sStor

//omitted ctors


@override
public void write(DataOutput out) throws IOException{
    out.writeChars(xStr);
    out.WriteChars(yStr);

    //WHAT SHOULD I DO FOR List&lt;Field&gt;

}

@override
public void readFields(DataInput in) throws IOException{
    xStr = in.readLine();
    yStr = in.readLine();

    //WHAT SHOULD I DO FOR List&lt;Field&gt;
}

}

public class SubStorage{
    public String x;
    public String y;
}

}
</code></pre>
<p>Following is the Field class:</p>
<pre><code class="java">public final class Field implements Comparable&lt;Field&gt;, Serializable {

    private String name;
    private DataType dataType;
    private Object value;
    private FieldType fieldType;


    public Field(){

    }



    public  Field(String name, DataType dataType, FieldType fieldType){
        this(name, dataType, null, fieldType);
    }

    public  Field(String name, DataType type, Object value, FieldType fieldType){
        this.name = name;
        this.dataType = type;
        this.value = value;
        this.fieldType = fieldType;
    }
}





public enum FieldType {
    PRI, LOOKUP, SCD, VERSION, OTHER
}



public enum DataType {

    UNDEFINED(4) {
        public int getSizeInBytes(Object value) {
            return STRING.getSizeInBytes(value);
        }
    },

    STRING(4) {
        public int getSizeInBytes(Object value) {
            if (value == null) {
                return 0;
            }
            return super.getSizeInBytes(value) + (value.toString().length() * 2); // length + chars
        }
    },

    INT(4),
    LONG(8),
    DOUBLE(8),
    DATETIME(8),
    BOOLEAN(1),
    BYTE(1),
    FLOAT(4),
    SHORT(2),
    CHAR(2),
    DATE(8),
    TIME(8),

    BLOB(0) {
        public int getSizeInBytes(Object value) {
            if (value == null) {
                return 0;
            }
            return ((byte[])value).length;
        }
    };

    private final int sizeInBytes;

    private DataType(int sizeInBytes) {
        this.sizeInBytes = sizeInBytes;
    }

    public int getSizeInBytes(Object value) {
        return sizeInBytes;
    }
}
</code></pre>
<p>Serializing collections is quite simple.</p>
<pre><code class="java">@Override
public void readFields(DataInput in) throws IOException {
    int size = in.readInt();
    list= new ArrayList&lt;Field&gt;(size);
    for(int i = 0; i &lt; size; i++){
        Field f = new Field();
        f.readFields(in);
        list.add(f);
    }
}

@Override
public void write(DataOutput out) throws IOException {
    out.writeInt(list.size());
    for (Field l : list) {
        l.write(out);
    }
}
</code></pre>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.notehub.cn/2015/09/21/dev/How to customize Writable class in Hadoop/" data-id="ciszf7ppr000be4s828gjy3vl" class="article-share-link">分享到</a>
      
        <a href="http://www.notehub.cn/2015/09/21/dev/How to customize Writable class in Hadoop/#ds-thread" class="article-comment-link">评论</a>
      
      
    </footer>
  </div>
  
</article>



    
      <article id="post-algo/murmurhash-md" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/09/03/algo/murmurhash-md/">MurmurHash</a>
    </h1>
  

        <div class="article-meta">
          
  <div class="article-date">
    <i class="fa fa-calendar"></i>
    <a href="/2015/09/03/algo/murmurhash-md/">
      <time datetime="2015-09-03T06:55:54.000Z" itemprop="datePublished">2015-09-03</time>
    </a>
  </div>


          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/Hash/">Hash</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
                
        <p>　　MurmurHash 是一种非加密型哈希函数，适用于一般的哈希检索操作。由Austin Appleby在2008年发明，并出现了多个变种，都已经发布到了公有领域(public domain)。与其它流行的哈希函数相比，对于规律性较强的key，MurmurHash的随机分布特征表现更良好。</p>
<h3 id="u53D8_u79CD"><a href="#u53D8_u79CD" class="headerlink" title="变种"></a>变种</h3><p>　　当前的版本是MurmurHash3， 能够产生出32-bit或128-bit哈希值。较早的MurmurHash2能产生32-bit或64-bit哈希值。对于大端存储和强制对齐的硬件环境有一个较慢的MurmurHash2可以用。MurmurHash2A 变种增加了Merkle–Damgård 构造，所以能够以增量方式调用。 有两个变种产生64-bit哈希值：MurmurHash64A，为64位处理器做了优化；MurmurHash64B，为32位处理器做了优化。MurmurHash2-160用于产生160-bit 哈希值，而MurmurHash1已经不再使用。</p>
<h3 id="u5B9E_u73B0"><a href="#u5B9E_u73B0" class="headerlink" title="实现"></a>实现</h3><p>　　最初的实现是C++的，但是被移植到了其他的流行语言上，包括 Python, C,C#, Perl, Ruby, PHP,Haskell,、Scala、Java和JavaScript等。这个算法已经被若干开源计划所采纳，最重要的有libstdc++ (4.6版)、Perl、nginx (不早于1.0.1版)、Rubinius、 libmemcached (Memcached的C语言客户端驱动)、maatkit、Hadoop、Kyoto Cabinet以及RaptorDB。</p>
<p>A sample C implementation follows:</p>
<pre><code class="C">uint32_t murmur3_32(const char *key, uint32_t len, uint32_t seed) {
    static const uint32_t c1 = 0xcc9e2d51;
    static const uint32_t c2 = 0x1b873593;
    static const uint32_t r1 = 15;
    static const uint32_t r2 = 13;
    static const uint32_t m = 5;
    static const uint32_t n = 0xe6546b64;

    uint32_t hash = seed;

    const int nblocks = len / 4;
    const uint32_t *blocks = (const uint32_t *) key;
    int i;
    for (i = 0; i &lt; nblocks; i++) {
        uint32_t k = blocks[i];
        k *= c1;
        k = (k &lt;&lt; r1) | (k &gt;&gt; (32 - r1));
        k *= c2;

        hash ^= k;
        hash = ((hash &lt;&lt; r2) | (hash &gt;&gt; (32 - r2))) * m + n;
    }

    const uint8_t *tail = (const uint8_t *) (key + nblocks * 4);
    uint32_t k1 = 0;

    switch (len &amp; 3) {
    case 3:
        k1 ^= tail[2] &lt;&lt; 16;
    case 2:
        k1 ^= tail[1] &lt;&lt; 8;
    case 1:
        k1 ^= tail[0];

        k1 *= c1;
        k1 = (k1 &lt;&lt; r1) | (k1 &gt;&gt; (32 - r1));
        k1 *= c2;
        hash ^= k1;
    }

    hash ^= len;
    hash ^= (hash &gt;&gt; 16);
    hash *= 0x85ebca6b;
    hash ^= (hash &gt;&gt; 13);
    hash *= 0xc2b2ae35;
    hash ^= (hash &gt;&gt; 16);

    return hash;
}
</code></pre>
<h3 id="u76F8_u5173_u8D44_u6E90_uFF1A"><a href="#u76F8_u5173_u8D44_u6E90_uFF1A" class="headerlink" title="相关资源："></a>相关资源：</h3><ol>
<li><a href="http://blog.csdn.net/yfkiss/article/details/7337382" target="_blank" rel="external">http://blog.csdn.net/yfkiss/article/details/7337382</a></li>
<li><a href="https://zh.wikipedia.org/wiki/Murmur%E5%93%88%E5%B8%8C" target="_blank" rel="external">https://zh.wikipedia.org/wiki/Murmur%E5%93%88%E5%B8%8C</a></li>
<li><a href="https://en.wikipedia.org/wiki/MurmurHash" target="_blank" rel="external">https://en.wikipedia.org/wiki/MurmurHash</a></li>
<li><a href="https://github.com/lijingpeng/java_util/tree/master/src/util/hash" target="_blank" rel="external">https://github.com/lijingpeng/java_util/tree/master/src/util/hash</a></li>
<li><a href="https://github.com/huichen/murmur/blob/master/murmur.go" target="_blank" rel="external">https://github.com/huichen/murmur/blob/master/murmur.go</a></li>
<li><a href="https://pypi.python.org/pypi/mmh3/2.0" target="_blank" rel="external">https://pypi.python.org/pypi/mmh3/2.0</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.notehub.cn/2015/09/03/algo/murmurhash-md/" data-id="ciszf7pq7000te4s8ibdnv01h" class="article-share-link">分享到</a>
      
        <a href="http://www.notehub.cn/2015/09/03/algo/murmurhash-md/#ds-thread" class="article-comment-link">评论</a>
      
      
    </footer>
  </div>
  
</article>



    
      <nav id="page-nav">
        <a class="extend prev" rel="prev" href="/page/13/">&laquo; 上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="page-number" href="/page/13/">13</a><span class="page-number current">14</span><a class="page-number" href="/page/15/">15</a><a class="page-number" href="/page/16/">16</a><span class="space">&hellip;</span><a class="page-number" href="/page/21/">21</a><a class="extend next" rel="next" href="/page/15/">下一页 &raquo;</a>
      </nav>
    </section>
      
        <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul id="recent-post" class="">
        
          <li>
            
            <div class="item-thumbnail">
              <a href="/2016/09/09/algo/kaggle/gbid/" class="thumbnail">
  
    <span class="thumbnail-image thumbnail-none"></span>
  
</a>
            </div>
            
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/kaggle/">kaggle</a></p>
              <p class="item-title"><a href="/2016/09/09/algo/kaggle/gbid/" class="title">kaggle之Grupo Bimbo Inventory Demand</a></p>
              <p class="item-date"><time datetime="2016-09-09T05:48:13.000Z" itemprop="datePublished">2016-09-09</time></p>
            </div>
          </li>
        
          <li>
            
            <div class="item-thumbnail">
              <a href="/2016/09/08/dev/python/pandas_reset_index/" class="thumbnail">
  
    <span class="thumbnail-image thumbnail-none"></span>
  
</a>
            </div>
            
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/dev/">dev</a></p>
              <p class="item-title"><a href="/2016/09/08/dev/python/pandas_reset_index/" class="title">pandas reset_index</a></p>
              <p class="item-date"><time datetime="2016-09-08T02:19:56.000Z" itemprop="datePublished">2016-09-08</time></p>
            </div>
          </li>
        
          <li>
            
            <div class="item-thumbnail">
              <a href="/2016/08/31/dev/python/python_print_chs/" class="thumbnail">
  
    <span class="thumbnail-image thumbnail-none"></span>
  
</a>
            </div>
            
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/dev/">dev</a></p>
              <p class="item-title"><a href="/2016/08/31/dev/python/python_print_chs/" class="title">Python print 中文乱码问题</a></p>
              <p class="item-date"><time datetime="2016-08-31T02:19:56.000Z" itemprop="datePublished">2016-08-31</time></p>
            </div>
          </li>
        
          <li>
            
            <div class="item-thumbnail">
              <a href="/2016/08/26/algo/kaggle/facial_keypoints/" class="thumbnail">
  
    <span class="thumbnail-image thumbnail-none"></span>
  
</a>
            </div>
            
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/kaggle/">kaggle</a></p>
              <p class="item-title"><a href="/2016/08/26/algo/kaggle/facial_keypoints/" class="title">kaggle之人脸特征识别</a></p>
              <p class="item-date"><time datetime="2016-08-26T05:48:13.000Z" itemprop="datePublished">2016-08-26</time></p>
            </div>
          </li>
        
          <li>
            
            <div class="item-thumbnail">
              <a href="/2016/08/25/algo/ml/hmm_crf/" class="thumbnail">
  
    <span style="background-image:url(/images/条件随机场.png
)" alt="一张关于贝叶斯、HMM、CRF的图" class="thumbnail-image"></span>
  
</a>
            </div>
            
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
              <p class="item-title"><a href="/2016/08/25/algo/ml/hmm_crf/" class="title">一张关于贝叶斯、HMM、CRF的图</a></p>
              <p class="item-date"><time datetime="2016-08-24T16:00:00.000Z" itemprop="datePublished">2016-08-25</time></p>
            </div>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hadoop/">Hadoop</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hash/">Hash</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hive/">Hive</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Photo/">Photo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/SQL/">SQL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Zabbix/">Zabbix</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ads/">ads</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/c/">c++</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/dev/">dev</a><span class="category-list-count">31</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/github/">github</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/internet/">internet</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/kaggle/">kaggle</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">20</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/other/">other</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/sklearn/">sklearn</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/system/">system</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/travel/">travel</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/web/">web</a><span class="category-list-count">4</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Apache/" style="font-size: 10px;">Apache</a> <a href="/tags/Bandits/" style="font-size: 12.5px;">Bandits</a> <a href="/tags/Elasticsearch-docker/" style="font-size: 10px;">Elasticsearch, docker</a> <a href="/tags/FlatBuffers/" style="font-size: 10px;">FlatBuffers</a> <a href="/tags/Google/" style="font-size: 10px;">Google</a> <a href="/tags/Granger-causality/" style="font-size: 10px;">Granger causality</a> <a href="/tags/GridSearchCV/" style="font-size: 10px;">GridSearchCV</a> <a href="/tags/HTTPS/" style="font-size: 10px;">HTTPS</a> <a href="/tags/Jquery/" style="font-size: 10px;">Jquery</a> <a href="/tags/MapReduce/" style="font-size: 10px;">MapReduce</a> <a href="/tags/Mock/" style="font-size: 10px;">Mock</a> <a href="/tags/Monument-Vallay/" style="font-size: 10px;">Monument Vallay</a> <a href="/tags/Mysql/" style="font-size: 12.5px;">Mysql</a> <a href="/tags/PPTP-vpn/" style="font-size: 12.5px;">PPTP, vpn</a> <a href="/tags/Photoshop/" style="font-size: 10px;">Photoshop</a> <a href="/tags/Pipeline/" style="font-size: 10px;">Pipeline</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/ad/" style="font-size: 10px;">ad</a> <a href="/tags/apache/" style="font-size: 10px;">apache</a> <a href="/tags/blade/" style="font-size: 10px;">blade</a> <a href="/tags/crontab/" style="font-size: 10px;">crontab</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/elasticsearch/" style="font-size: 12.5px;">elasticsearch</a> <a href="/tags/font/" style="font-size: 10px;">font</a> <a href="/tags/github/" style="font-size: 10px;">github</a> <a href="/tags/go/" style="font-size: 17.5px;">go</a> <a href="/tags/hashmap/" style="font-size: 10px;">hashmap</a> <a href="/tags/kaggle/" style="font-size: 20px;">kaggle</a> <a href="/tags/linux-ldd-依赖关系/" style="font-size: 10px;">linux, ldd, 依赖关系</a> <a href="/tags/lucene/" style="font-size: 10px;">lucene</a> <a href="/tags/nio/" style="font-size: 10px;">nio</a> <a href="/tags/pagerank/" style="font-size: 10px;">pagerank</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/swift-llvm/" style="font-size: 10px;">swift, llvm</a> <a href="/tags/前端/" style="font-size: 10px;">前端</a> <a href="/tags/广告/" style="font-size: 12.5px;">广告</a> <a href="/tags/开源，许可/" style="font-size: 10px;">开源，许可</a> <a href="/tags/澳门/" style="font-size: 10px;">澳门</a> <a href="/tags/监控，zabbix/" style="font-size: 10px;">监控，zabbix</a> <a href="/tags/贝叶斯/" style="font-size: 10px;">贝叶斯</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">五月 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">三月 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">二月 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">一月 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">十二月 2015</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">十一月 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">十月 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">九月 2015</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">六月 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">五月 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">四月 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">三月 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">二月 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">一月 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">十二月 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">十一月 2014</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/10/">十月 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/06/">六月 2014</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/11/">十一月 2013</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
  <div id="toTop" class="fa fa-chevron-up"></div>
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Li Jingpeng<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="http://github.com/ppoffice">PPOffice</a>
    </div>
  </div>
</footer>
    

<script type="text/javascript">
  var duoshuoQuery = {short_name:"lijingpeng"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script>


<script src="//ajax.css.network/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>