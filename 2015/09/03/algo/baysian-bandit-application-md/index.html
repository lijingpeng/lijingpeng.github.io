<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    
    <title>Bayesian Bandits原理及在互联网广告行业的应用 | Frank</title>
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="喜欢足球">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Bayesian Bandits原理及在互联网广告行业的应用 | Frank">
    <meta name="twitter:description" content="喜欢足球">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Bayesian Bandits原理及在互联网广告行业的应用 | Frank">
    <meta property="og:description" content="喜欢足球">

    
    <meta name="author" content="Li Jingpeng">
    
    <link rel="stylesheet" href="/css/vno.css" type="text/css">
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" type="text/css">

    
    <link rel="icon" href="/images/favicon.png">
    

    <meta name="generator" content="hexo"/>
    
    <link rel="alternate" type="application/rss+xml" title="Frank" href="/atom.xml">
    

    <link rel="canonical" href="http://yoursite.com/2015/09/03/algo/baysian-bandit-application-md/"/>

    
</head>

<body class="home-template no-js">

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>

    
<header class="panel-cover panel-cover--collapsed" style="background-image: url(/images/background-cover.jpg)">
  <div class="panel-main">
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/" title="前往 Frank 的主页"><img src="/images/logo2.jpg" width="80" alt="Frank logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage for Frank">Frank</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">Li Jingpeng&#39;s site</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">计算机 网络 互联网</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />

        <div class="navigation-wrapper">
          <div>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              <li class="navigation__item"><a href="/#blog" title="访问博客" class="blog-button">博客</a></li>
            
              <li class="navigation__item"><a href="/">主页</a></li>
            
              <li class="navigation__item"><a href="/aboutme">关于我</a></li>
            
            </ul>
          </nav>
          </div>
          <div>
          <nav class="cover-navigation navigation--social">
  <ul class="navigation">

  <!-- Weibo-->
  
  <li class="navigation__item">
    <a href="http://weibo.com/329299516" title="我的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li> 


  <!-- Github -->
  
  <li class="navigation__item">
    <a href="https://github.com/lijingpeng" title="查看我的GitHub主页" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>


<!-- Stack Overflow -->
        

  <!-- Google Plus -->
  

<!-- Facebook -->

  
<!-- Twitter -->

  

  <li class="navigation__item">
    <a href="/atom.xml" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>


  <li class="navigation__item">
    <a href="mailto:me@lijingpeng.org" title="邮件联系我" target="_blank">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>


  </ul>
</nav>

          </div>
        </div>

      </div>

    </div>

    <div class="panel-cover--overlay cover-blue"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single">

  <header class="post-header">
    <div class="post-meta">
      <time datetime="2015-09-03T05:48:13.000Z" class="post-list__meta--date date">2015-09-03</time> &#8226; <span class="post-meta__tags tags">于&nbsp;</span>
    </div>
    <h1 class="post-title">Bayesian Bandits原理及在互联网广告行业的应用</h1>
  </header>

  <section class="post">
    <h2 id="1-_The_Multi-Armed_Bandit_Problem">1. The Multi-Armed Bandit Problem</h2><p>Suppose you are faced with N slot machines (colourfully called multi-armed bandits). Each bandit has an unknown probability of distributing a prize (assume for now the prizes are the same for each bandit, only the probabilities differ). Some bandits are very generous, others not so much. Of course, you don’t know what these probabilities are. By only choosing one bandit per round, our task is devise a strategy to maximize our winnings.</p>
<p>Of course, if we knew the bandit with the largest probability, then always picking this bandit would yield the maximum winnings. So our task can be phrased as “Find the best bandit, and as quickly as possible”.</p>
<p>The task is complicated by the stochastic nature of the bandits. A suboptimal bandit can return many winnings, purely by chance, which would make us believe that it is a very profitable bandit. Similarly, the best bandit can return many duds. Should we keep trying losers then, or give up?</p>
<p>A more troublesome problem is, if we have a found a bandit that returns pretty good results, do we keep drawing from it to maintain our pretty good score, or do we try other bandits in hopes of finding an even-better bandit? This is the exploration vs. exploitation dilemma.</p>
<h2 id="2-_Applications">2. Applications</h2><p>The Multi-Armed Bandit problem at first seems very artificial, something only a mathematician would love, but that is only before we address some applications:</p>
<p>Internet display advertising: companies have a suite of potential ads they can display to visitors, but the company is not sure which ad strategy to follow to maximize sales. This is similar to A/B testing, but has the added advantage of naturally minimizing strategies that do not work (and generalizes to A/B/C/D… strategies)</p>
<ol>
<li>Ecology: animals have a finite amount of energy to expend, and following certain behaviours has uncertain rewards. How does the animal maximize its fitness?</li>
<li>Finance: which stock option gives the highest return, under time-varying return profiles.</li>
<li>Clinical trials: a researcher would like to find the best treatment, out of many possible treatments, while minimizing losses.</li>
</ol>
<p>Many of these questions above are fundamental to the application’s field. It turns out the optimal solution is incredibly difficult, and it took decades for an overall solution to develop. There are also many approximately-optimal solutions which are quite good. The one I wish to discuss is one of the few solutions that can scale incredibly well. The solution is known asBayesian Bandits.</p>
<h2 id="3-_A_Proposed_Solution">3. A Proposed Solution</h2><p>Any proposed strategy is called an online algorithm (not in the internet sense, but in the continuously-being-updated sense), and more specifically a reinforcement learning algorithm. The algorithm starts in an ignorant state, where it knows nothing, and begins to acquire data by testing the system. As it acquires data and results, it learns what the best and worst behaviours are (in this case, it learns which bandit is the best). With this in mind, perhaps we can add an additional application of the Multi-Armed Bandit problem:</p>
<p>Psychology: how does punishment and reward effect our behaviour? How do humans’ learn?<br>The Bayesian solution begins by assuming priors on the probability of winning for each bandit. In our vignette we assumed complete ignorance of the these probabilities. So a very natural prior is the flat prior over 0 to 1. The algorithm proceeds as follows:</p>
<p>For each round,</p>
<ol>
<li>Sample a random variable Xb from the prior of bandit b, for all b.</li>
<li>Select the bandit with largest sample, i.e. select bandit B=argmaxXb.</li>
<li>Observe the result of pulling bandit B, and update your prior on bandit B.</li>
<li>Return to 1.</li>
</ol>
<p>That’s it. Computationally, the algorithm involves sampling from N distributions. Since the initial priors are Beta(α=1,β=1) (a uniform distribution), and the observed result X (a win or loss, encoded 1 and 0 respectfully) is Binomial, the posterior is a Beta(α=1+X,β=1+1−X)(see here for why to is true). </p>
<p>To answer a question from before, this algorithm suggests that we should not discard losers, but we should pick them at a decreasing rate as we gather confidence that there exist better bandits. This follows because there is always a non-zero chance that a loser will achieve the status of B, but the probability of this event decreases as we play more rounds (see figure below). Below is an implementation of the Bayesian Bandits strategy (which can be skipped for the less Pythonic-ly interested).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pymc <span class="keyword">import</span> rbeta</span><br><span class="line"> </span><br><span class="line">rand = np.random.rand</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bandits</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    This class represents N bandits machines.</span><br><span class="line"> </span><br><span class="line">    parameters:</span><br><span class="line">        p_array: a (n,) Numpy array of probabilities &gt;0, &lt;1.</span><br><span class="line"> </span><br><span class="line">    methods:</span><br><span class="line">        pull( i ): return the results, 0 or 1, of pulling </span><br><span class="line">                   the ith bandit.</span><br><span class="line">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, p_array)</span>:</span></span><br><span class="line">        self.p = p_array</span><br><span class="line">        self.optimal = np.argmax(p_array)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pull</span><span class="params">( self, i )</span>:</span></span><br><span class="line">        <span class="comment">#i is which arm to pull</span></span><br><span class="line">        <span class="keyword">return</span> rand() &lt; self.p[i]</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.p)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BayesianStrategy</span><span class="params">( object )</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Implements a online, learning strategy to solve</span><br><span class="line">    the Multi-Armed Bandit problem.</span><br><span class="line"> </span><br><span class="line">    parameters:</span><br><span class="line">        bandits: a Bandit class with .pull method</span><br><span class="line"> </span><br><span class="line">    methods:</span><br><span class="line">        sample_bandits(n): sample and train on n pulls.</span><br><span class="line"> </span><br><span class="line">    attributes:</span><br><span class="line">        N: the cumulative number of samples</span><br><span class="line">        choices: the historical choices as a (N,) array</span><br><span class="line">        bb_score: the historical score as a (N,) array</span><br><span class="line"> </span><br><span class="line">    """</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bandits)</span>:</span></span><br><span class="line"> </span><br><span class="line">        self.bandits = bandits</span><br><span class="line">        n_bandits = len( self.bandits )</span><br><span class="line">        self.wins = np.zeros( n_bandits )</span><br><span class="line">        self.trials = np.zeros(n_bandits )</span><br><span class="line">        self.N = <span class="number">0</span></span><br><span class="line">        self.choices = []</span><br><span class="line">        self.bb_score = []</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample_bandits</span><span class="params">( self, n=<span class="number">1</span> )</span>:</span></span><br><span class="line"> </span><br><span class="line">        bb_score = np.zeros( n )</span><br><span class="line">        choices = np.zeros( n )</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment">#sample from the bandits's priors, and select the largest sample</span></span><br><span class="line">            choice = np.argmax( rbeta( <span class="number">1</span> + self.wins, <span class="number">1</span> + self.trials - self.wins) )</span><br><span class="line"> </span><br><span class="line">            <span class="comment">#sample the chosen bandit</span></span><br><span class="line">            result = self.bandits.pull( choice )</span><br><span class="line"> </span><br><span class="line">            <span class="comment">#update priors and score</span></span><br><span class="line">            self.wins[ choice ] += result</span><br><span class="line">            self.trials[ choice ] += <span class="number">1</span></span><br><span class="line">            bb_score[ k ] = result </span><br><span class="line">            self.N += <span class="number">1</span></span><br><span class="line">            choices[ k ] = choice</span><br><span class="line"> </span><br><span class="line">        self.bb_score = np.r_[ self.bb_score, bb_score ]</span><br><span class="line">        self.choices = np.r_[ self.choices, choices ]</span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
<p>Below we present a visualization of the algorithm sequentially learning the solution. In the figure below, the dashed lines represent the true hidden probabilities, which are (0.85, 0.60, 0.75)(this can be extended to many more dimensions, but the figure suffers, so I kept it at 3).</p>
<p><img src="/images/bb/updating2.png" alt=""></p>
<p>Note that we don’t real care how accurate we become about inference of the hidden probabilities — for this problem we are more interested in choosing the best bandit (or more accurately, becoming more confident in choosing the best bandit). For this reason, the distribution of the red bandit is very wide (representing ignorance about what that hidden probability might be) but we are reasonably confident that it is not the best, so the algorithm chooses to ignore it.</p>
<h3 id="几篇介绍Bayesian_Bandits的原理的文章：">几篇介绍Bayesian Bandits的原理的文章：</h3><ol>
<li><a href="https://www.chrisstucchio.com/blog/2013/bayesian_analysis_conversion_rates.html" target="_blank" rel="external">https://www.chrisstucchio.com/blog/2013/bayesian_analysis_conversion_rates.html</a></li>
<li>在线演示博弈过程： <a href="https://e76d6ebf22ef8d7e079810f3d1f82ba1e5f145d5.googledrive.com/host/0B2GQktu-wcTiWDB2R2t2a2tMUG8/" target="_blank" rel="external">https://e76d6ebf22ef8d7e079810f3d1f82ba1e5f145d5.googledrive.com/host/0B2GQktu-wcTiWDB2R2t2a2tMUG8/</a></li>
<li><a href="https://www.chrisstucchio.com/blog/2013/bayesian_bandit.html" target="_blank" rel="external">https://www.chrisstucchio.com/blog/2013/bayesian_bandit.html</a></li>
<li><a href="http://camdp.com/blogs/multi-armed-bandits" target="_blank" rel="external">http://camdp.com/blogs/multi-armed-bandits</a></li>
<li>贝叶斯书籍：<a href="https://github.com/lijingpeng/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers" target="_blank" rel="external">https://github.com/lijingpeng/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers</a></li>
</ol>

  </section>

</article>


<section class="post-comments">
  <!-- 多说评论框 start -->
  <div class="ds-thread" data-thread-key="http://yoursite.com/2015/09/03/algo/baysian-bandit-application-md/" data-title="Bayesian Bandits原理及在互联网广告行业的应用" data-url="http://yoursite.com/2015/09/03/algo/baysian-bandit-application-md/"></div>
  <!-- 多说评论框 end -->
  <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
  <script type="text/javascript">
  var duoshuoQuery = {short_name:"lijingpeng"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共JS代码 end -->
</section>



            <footer class="footer">
    <span class="footer__copyright">
        本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>
    </span>
    <span class="footer__copyright">
        基于 <a href="http://hexo.io">Hexo</a> 搭建，感谢 <a href="https://pages.github.com/">GitHub Pages</a> 提供免费的托管服务
    </span>
    <span class="footer__copyright">
        &copy; 2015 - 本站由 <a href="/">@Longbo Ma</a> 创建,
        使用<a href="https://github.com/lenbo-ma/hexo-theme-vno">hexo-theme-vno</a>主题,
        修改自<a href="http://github.com/onevcat/vno" target="_blank">Vno</a>
    </span>
</footer>

        </div>
    </div>

    <script src="http://cdn.bootcss.com/jquery/2.1.4/jquery.min.js" type="text/javascript"></script>
    <script src="/js/main.js" type="text/javascript"></script>

     
</body>
</html>
